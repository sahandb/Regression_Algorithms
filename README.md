# Regression Algorithms
Implementation of four Regression Model from scratch

At first I create 4 data model with instructions and then use them in our regression models
Experiments on regression problems use 4 different sets of synthetic data with ğ‘› data points {ğ‘¥ğ‘– }ğ‘–-ğ‘› with ğ‘ dimensions.

Stack these data points in a ğ‘ Ã— ğ‘› matrix of ğ‘‹. The univariate response variable ğ’š depends only on a particular set of features. The ğ‘—th feature is denoted by ğ‘‹ğ‘—: (ğ‘—th row of ğ‘‹) and the ğ‘–th data point is denoted by ğ‘‹:ğ‘– (ğ‘–th column of ğ‘‹).

Regression A: This regression model is defined as:

![image](https://user-images.githubusercontent.com/24508376/219643938-4cb1b901-52b2-48a9-af17-7ce592671720.png)

where ğ‘‹:ğ‘– âˆ¼ ğ‘(0, ğ¼4) is a four-dimensional input vector and ğœ€ âˆ¼ ğ‘(0,1) is the normal additive Gaussian noise. In this model only, the first two features are related to the response variable ğ’š.

Regression B: The second regression model is as follows:

![image](https://user-images.githubusercontent.com/24508376/219643989-40fe5304-c7d3-43c8-99a6-b4228aa85d7f.png)

where ğ‘‹:ğ‘– âˆ¼ ğ‘(0, ğ¼10) $ and ğœ€ âˆ¼ ğ‘(0,1) is the independent noise. The dimension of the true space is 1 and the noise is multiplicative rather than additive in this case.

Regression C: The third regression model is defined as follows:

![image](https://user-images.githubusercontent.com/24508376/219644221-f60b4fe5-c441-4feb-91b2-bf849caab38f.png)


where ğ‘‹ âˆ¼ ğ‘(0, ğ¼4) is as it is defined in regression (A), and ğœ€ âˆ¼ ğ‘(0,1).

Regression D: The final regression model is as follows:

![image](https://user-images.githubusercontent.com/24508376/219644389-56402b01-abcf-4329-b0cb-c4a7e92c0d20.png)


in which ğ‘‹ = (ğ‘‹1, . . . , ğ‘‹10)ğ‘‡ âˆ¼ ğ‘(0, ğ¼10), and ğœ€ âˆ¼ ğ‘(0,1).

Samples of size ğ‘› = 100 drawn out of each regression model and in each set 70%, of the data is used for training and the remaining 30% is used as testing data. The average root mean square error (RMSE) of estimating the test data response variable for different regression models should be reported in each case for fifty times.

After constructing these regression models, you are required to implement the following four algorithms.

I. Linear Regression

First objective model is multiple linear regression


![image](https://user-images.githubusercontent.com/24508376/219645018-60d17bb0-cd0c-4a8d-80e5-b874a502212b.png)


II. Ridge Regression (Regression with ğ‘³ğŸ-regularization )

Second one is Ridge regression as we know lambda is regularization penalty


As you know, when a regularization penalty (scaled by Î») is added to the objective function of linear regression, we call it ridge regression, and its objective function is:

![image](https://user-images.githubusercontent.com/24508376/219645218-c97f2835-d6c8-4672-8c07-b88b61be0a5d.png)

which can be solved in closed-form as:

![image](https://user-images.githubusercontent.com/24508376/219645280-e2c76a22-0b03-47cf-bba6-464ad633330d.png)


  III. Coordinate Descent for Lasso
  
  Third is Lasso regression and we donâ€™t have any closed form bcz we cant differentiate from objective function then for that then we use coordinate decent, in which at each step we optimize over one component of the unknown parameter vector and fix all other unknown components. Aka shooting algorithm.

  
  The Lasso optimization problem can be formulated as:
  
  ![image](https://user-images.githubusercontent.com/24508376/219645404-220b7b2c-0fca-474d-b7b4-93964cd32d0a.png)


in which
![image](https://user-images.githubusercontent.com/24508376/219645453-c4f988c3-2ae8-45ea-8c9a-5f0e60d06e9d.png)


Since the ğ¿1-regularization term in the objective function is non-differentiable, it is not clear how gradient descent or SGD could be used to solve this optimization problem, directly. Another approach to solve an optimization problem is coordinate descent, in which at each step we optimize over one component of the unknown parameter vector and fix all other unknown components. The descent path is a sequence of steps, each of which is parallel to a coordinate axis in ğ‘…ğ‘‘ .

This gives us the following algorithm, known as the shooting algorithm:

![image](https://user-images.githubusercontent.com/24508376/219645590-71e56871-5b85-4f99-911e-187ea1fcf6df.png)

(Source: Murphy, Kevin P. Machine learning: a probabilistic perspective. MIT press, 2012.)

The â€œsoft thresholdingâ€ function is defined as:

ğ‘ ğ‘œğ‘“ğ‘¡(ğ‘, ğ›¿) = ğ‘ ğ‘–ğ‘”ğ‘›(ğ‘)(|ğ‘| âˆ’ ğ›¿)+

where (|ğ‘| âˆ’ ğ›¿)+ = max ((|ğ‘| âˆ’ ğ›¿), 0) is the positive part of (|ğ‘| âˆ’ ğ›¿).

IV. Kernelized Ridge Regression

Four is kernel ridge regression that we multiple all sample with a kernel (gaussian or poly) over all samples and then find parameter (a) and then with that a we predict the test samples.


We replace all data points with their feature vector: ğ‘¥ğ‘– â†’ Î¦ğ‘– = Î¦(ğ‘¥ğ‘– ). In this case, the number of dimensions can be much higher, or even infinitely higher, than the number of data points. There is a neat trick that allows us to perform ridge regression in high-dimensional space as follows:

![image](https://user-images.githubusercontent.com/24508376/219645782-72738094-4df1-4547-a9a3-96605019f6fd.png)

where ğ¾(ğ‘¥, ğ‘¥ğ‘– ) = ğ›·(ğ‘¥)ğ›·(ğ‘¥ğ‘– )ğ‘‡.

I applied four mentioned methods on four generated data sets and report the RMSE for different values of the regularization parameter ğœ† = {0.5, 1, 10, 100, 1000}.


# Some important questions here:

Explain advantages and disadvantages of kernel ridge regression? Under what circumstances is the kernel regression better than the other methods?
Thatâ€™s a common approach to map samples to a high dimensional space using a nonlinear mapping, and then learn the model in the high dimensional space and disadvantage is computational complexity
Under this circumstance :Learning non-linear relationships between predictor variables and responses is a fundamental problem in machine learning .One state-of-the-art method is Kernel Ridge Regression (KRR)

What if ğœ† is set to an extremely large value? Explain the role of the regularization parameter ğœ† on training phase.
To solve some ill-posed problems or prevent overfitting, L2 regularization is used, as formulated in Ridge Regression. Î» is a positive parameter that controls w: the larger is Î», the smaller is âˆ¥wâˆ¥2

In Kernel ridge regression, which kernel is better than the other, and why?
Gaussian is better because it has sigma parameter and lambda that leads to more complex model than polynomial and it has more generalization

When is lasso worse than ridge regression?
Ridge tends to give small but well distributed weights, because the l2 regularization cares more about driving big weight to small weights, instead of driving small weights to zeros. If you only have a few predictors, and you are confident that all of them should be really relevant for predictions, try Ridge as a good regularized linear regression method.

Compare four mentioned methods, in terms of, noise, outlier, the linearly separable data, the non-linearly separable data, number of samples, and number of features. What is the effect of ğœ€ on each regression algorithm?






